
==> Audit <==
|------------|---------------------|----------|---------|---------|---------------------|---------------------|
|  Command   |        Args         | Profile  |  User   | Version |     Start Time      |      End Time       |
|------------|---------------------|----------|---------|---------|---------------------|---------------------|
| start      | --driver=virtualbox | minikube | haritha | v1.34.0 | 08 Dec 24 15:21 UTC |                     |
| start      | --driver=docker     | minikube | haritha | v1.34.0 | 08 Dec 24 15:21 UTC |                     |
| dashboard  |                     | minikube | haritha | v1.34.0 | 08 Dec 24 15:22 UTC |                     |
| start      |                     | minikube | haritha | v1.34.0 | 08 Dec 24 15:22 UTC |                     |
| start      | --driver=docker     | minikube | haritha | v1.34.0 | 08 Dec 24 15:23 UTC | 08 Dec 24 15:34 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 08 Jan 25 18:37 UTC | 08 Jan 25 18:37 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 17 Jan 25 05:14 UTC |                     |
| docker-env | minikube docker-env | minikube | haritha | v1.34.0 | 17 Jan 25 05:26 UTC | 17 Jan 25 05:26 UTC |
| docker-env | minikube docker-env | minikube | haritha | v1.34.0 | 17 Jan 25 05:26 UTC | 17 Jan 25 05:26 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 17 Jan 25 18:38 UTC |                     |
| stop       |                     | minikube | haritha | v1.34.0 | 17 Jan 25 18:49 UTC | 17 Jan 25 18:50 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 17 Jan 25 18:50 UTC |                     |
| delete     | --all               | minikube | haritha | v1.34.0 | 17 Jan 25 18:55 UTC | 17 Jan 25 18:55 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 17 Jan 25 18:55 UTC | 17 Jan 25 18:56 UTC |
| docker-env |                     | minikube | haritha | v1.34.0 | 17 Jan 25 18:59 UTC | 17 Jan 25 18:59 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 17 Jan 25 19:09 UTC |                     |
| stop       |                     | minikube | haritha | v1.34.0 | 17 Jan 25 19:09 UTC | 17 Jan 25 19:09 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 17 Jan 25 19:16 UTC | 17 Jan 25 19:16 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 18 Jan 25 04:51 UTC | 18 Jan 25 04:52 UTC |
| docker-env |                     | minikube | haritha | v1.34.0 | 18 Jan 25 04:53 UTC | 18 Jan 25 04:53 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 18 Jan 25 04:54 UTC | 18 Jan 25 04:55 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 20 Jan 25 16:37 UTC | 20 Jan 25 16:37 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 22 Jan 25 15:22 UTC | 22 Jan 25 15:22 UTC |
| ssh        |                     | minikube | haritha | v1.34.0 | 22 Jan 25 16:27 UTC |                     |
| service    | demo                | minikube | haritha | v1.34.0 | 22 Jan 25 16:43 UTC |                     |
| service    | demo                | minikube | haritha | v1.34.0 | 22 Jan 25 18:48 UTC |                     |
| start      |                     | minikube | haritha | v1.34.0 | 29 Jan 25 18:17 UTC | 29 Jan 25 18:18 UTC |
| service    | demo --url          | minikube | haritha | v1.34.0 | 29 Jan 25 18:31 UTC | 29 Jan 25 18:32 UTC |
| service    | demo --url          | minikube | haritha | v1.34.0 | 29 Jan 25 18:44 UTC | 29 Jan 25 18:50 UTC |
| service    | demo --url          | minikube | haritha | v1.34.0 | 29 Jan 25 18:52 UTC | 29 Jan 25 18:54 UTC |
| tunnel     |                     | minikube | haritha | v1.34.0 | 29 Jan 25 18:54 UTC | 29 Jan 25 18:56 UTC |
| service    | demo --url          | minikube | haritha | v1.34.0 | 29 Jan 25 18:58 UTC | 29 Jan 25 19:03 UTC |
| service    | demo --url          | minikube | haritha | v1.34.0 | 29 Jan 25 19:06 UTC |                     |
| start      |                     | minikube | haritha | v1.34.0 | 01 Feb 25 18:00 UTC | 01 Feb 25 18:01 UTC |
| service    | demo --url          | minikube | haritha | v1.34.0 | 01 Feb 25 18:06 UTC | 01 Feb 25 18:07 UTC |
| tunnel     |                     | minikube | haritha | v1.34.0 | 01 Feb 25 18:07 UTC | 01 Feb 25 18:08 UTC |
| service    | demo --url          | minikube | haritha | v1.34.0 | 01 Feb 25 18:11 UTC | 01 Feb 25 18:12 UTC |
| ip         |                     | minikube | haritha | v1.34.0 | 01 Feb 25 18:14 UTC | 01 Feb 25 18:14 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 01 Feb 25 18:20 UTC | 01 Feb 25 18:21 UTC |
| service    | demo --url          | minikube | haritha | v1.34.0 | 01 Feb 25 18:23 UTC | 01 Feb 25 18:23 UTC |
| service    | demo --url          | minikube | haritha | v1.34.0 | 01 Feb 25 18:26 UTC | 01 Feb 25 18:40 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 01 Feb 25 20:09 UTC | 01 Feb 25 20:09 UTC |
| service    | demo --url          | minikube | haritha | v1.34.0 | 01 Feb 25 20:39 UTC | 01 Feb 25 20:42 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 01 Feb 25 21:12 UTC | 01 Feb 25 21:13 UTC |
| start      |                     | minikube | haritha | v1.34.0 | 01 Feb 25 21:52 UTC | 01 Feb 25 21:53 UTC |
| service    | demo1 --url         | minikube | haritha | v1.34.0 | 01 Feb 25 22:02 UTC |                     |
| service    | demo1 --url         | minikube | haritha | v1.34.0 | 01 Feb 25 22:03 UTC |                     |
| service    | demo1 --url         | minikube | haritha | v1.34.0 | 01 Feb 25 22:03 UTC |                     |
| service    | demo1 --url         | minikube | haritha | v1.34.0 | 01 Feb 25 22:05 UTC |                     |
| service    | demo1 --url         | minikube | haritha | v1.34.0 | 01 Feb 25 22:06 UTC |                     |
| start      |                     | minikube | haritha | v1.34.0 | 01 Feb 25 22:08 UTC | 01 Feb 25 22:08 UTC |
| service    | demo1 --url         | minikube | haritha | v1.34.0 | 01 Feb 25 22:09 UTC |                     |
|------------|---------------------|----------|---------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/02/01 22:08:19
Running on machine: DESKTOP-732U35G
Binary: Built with gc go1.22.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0201 22:08:19.192796   10079 out.go:345] Setting OutFile to fd 1 ...
I0201 22:08:19.192998   10079 out.go:397] isatty.IsTerminal(1) = true
I0201 22:08:19.193002   10079 out.go:358] Setting ErrFile to fd 2...
I0201 22:08:19.193004   10079 out.go:397] isatty.IsTerminal(2) = true
I0201 22:08:19.193150   10079 root.go:338] Updating PATH: /home/haritha/.minikube/bin
I0201 22:08:19.196809   10079 out.go:352] Setting JSON to false
I0201 22:08:19.202115   10079 start.go:129] hostinfo: {"hostname":"DESKTOP-732U35G","uptime":15090,"bootTime":1738432610,"procs":79,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"5.15.167.4-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"daa95ffe-7ac2-415c-92cb-395473c6e6cd"}
I0201 22:08:19.202220   10079 start.go:139] virtualization:  guest
I0201 22:08:19.204532   10079 out.go:177] 😄  minikube v1.34.0 on Ubuntu 24.04 (amd64)
I0201 22:08:19.211314   10079 notify.go:220] Checking for updates...
I0201 22:08:19.211483   10079 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0201 22:08:19.212613   10079 driver.go:394] Setting default libvirt URI to qemu:///system
I0201 22:08:19.348553   10079 docker.go:123] docker version: linux-27.5.0:Docker Engine - Community
I0201 22:08:19.348768   10079 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0201 22:08:19.550853   10079 info.go:266] docker info: {ID:460110b0-6688-48d0-bd78-58b58ce1cbe6 Containers:40 ContainersRunning:2 ContainersPaused:0 ContainersStopped:38 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:32 OomKillDisable:true NGoroutines:52 SystemTime:2025-02-01 22:08:19.530796863 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4039925760 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-732U35G Labels:[] ExperimentalBuild:false ServerVersion:27.5.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.19.3] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4]] Warnings:<nil>}}
I0201 22:08:19.551211   10079 docker.go:318] overlay module found
I0201 22:08:19.554034   10079 out.go:177] ✨  Using the docker driver based on existing profile
I0201 22:08:19.555886   10079 start.go:297] selected driver: docker
I0201 22:08:19.555894   10079 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/haritha:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0201 22:08:19.555968   10079 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0201 22:08:19.556302   10079 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0201 22:08:19.615913   10079 info.go:266] docker info: {ID:460110b0-6688-48d0-bd78-58b58ce1cbe6 Containers:40 ContainersRunning:2 ContainersPaused:0 ContainersStopped:38 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:32 OomKillDisable:true NGoroutines:52 SystemTime:2025-02-01 22:08:19.603193893 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4039925760 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-732U35G Labels:[] ExperimentalBuild:false ServerVersion:27.5.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.19.3] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4]] Warnings:<nil>}}
I0201 22:08:19.618247   10079 cni.go:84] Creating CNI manager for ""
I0201 22:08:19.618291   10079 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0201 22:08:19.618357   10079 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/haritha:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0201 22:08:19.622583   10079 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0201 22:08:19.625592   10079 cache.go:121] Beginning downloading kic base image for docker with docker
I0201 22:08:19.627753   10079 out.go:177] 🚜  Pulling base image v0.0.45 ...
I0201 22:08:19.630972   10079 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I0201 22:08:19.631041   10079 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0201 22:08:19.631201   10079 preload.go:146] Found local preload: /home/haritha/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I0201 22:08:19.631296   10079 cache.go:56] Caching tarball of preloaded images
I0201 22:08:19.632418   10079 preload.go:172] Found /home/haritha/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0201 22:08:19.632467   10079 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I0201 22:08:19.632653   10079 profile.go:143] Saving config to /home/haritha/.minikube/profiles/minikube/config.json ...
W0201 22:08:19.677684   10079 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I0201 22:08:19.677738   10079 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0201 22:08:19.678212   10079 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I0201 22:08:19.678228   10079 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I0201 22:08:19.678232   10079 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I0201 22:08:19.678272   10079 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I0201 22:08:19.678276   10079 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I0201 22:08:20.631968   10079 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I0201 22:08:20.632006   10079 cache.go:194] Successfully downloaded all kic artifacts
I0201 22:08:20.632214   10079 start.go:360] acquireMachinesLock for minikube: {Name:mk9ca147f516e9ef09c9cb7fe3a05768953e154a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0201 22:08:20.632312   10079 start.go:364] duration metric: took 80.867µs to acquireMachinesLock for "minikube"
I0201 22:08:20.632391   10079 start.go:96] Skipping create...Using existing machine configuration
I0201 22:08:20.632458   10079 fix.go:54] fixHost starting: 
I0201 22:08:20.632713   10079 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0201 22:08:20.652262   10079 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0201 22:08:20.652281   10079 fix.go:138] unexpected machine state, will restart: <nil>
I0201 22:08:20.657100   10079 out.go:177] 🏃  Updating the running docker "minikube" container ...
I0201 22:08:20.659424   10079 machine.go:93] provisionDockerMachine start ...
I0201 22:08:20.659679   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:20.684096   10079 main.go:141] libmachine: Using SSH client type: native
I0201 22:08:20.684388   10079 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0201 22:08:20.684397   10079 main.go:141] libmachine: About to run SSH command:
hostname
I0201 22:08:20.810592   10079 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0201 22:08:20.810835   10079 ubuntu.go:169] provisioning hostname "minikube"
I0201 22:08:20.810938   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:20.829493   10079 main.go:141] libmachine: Using SSH client type: native
I0201 22:08:20.829762   10079 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0201 22:08:20.829771   10079 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0201 22:08:20.981138   10079 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0201 22:08:20.981417   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:20.997692   10079 main.go:141] libmachine: Using SSH client type: native
I0201 22:08:20.999538   10079 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0201 22:08:20.999551   10079 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0201 22:08:21.125355   10079 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0201 22:08:21.125537   10079 ubuntu.go:175] set auth options {CertDir:/home/haritha/.minikube CaCertPath:/home/haritha/.minikube/certs/ca.pem CaPrivateKeyPath:/home/haritha/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/haritha/.minikube/machines/server.pem ServerKeyPath:/home/haritha/.minikube/machines/server-key.pem ClientKeyPath:/home/haritha/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/haritha/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/haritha/.minikube}
I0201 22:08:21.125582   10079 ubuntu.go:177] setting up certificates
I0201 22:08:21.125600   10079 provision.go:84] configureAuth start
I0201 22:08:21.125782   10079 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0201 22:08:21.147660   10079 provision.go:143] copyHostCerts
I0201 22:08:21.147824   10079 exec_runner.go:144] found /home/haritha/.minikube/ca.pem, removing ...
I0201 22:08:21.147850   10079 exec_runner.go:203] rm: /home/haritha/.minikube/ca.pem
I0201 22:08:21.148036   10079 exec_runner.go:151] cp: /home/haritha/.minikube/certs/ca.pem --> /home/haritha/.minikube/ca.pem (1078 bytes)
I0201 22:08:21.149404   10079 exec_runner.go:144] found /home/haritha/.minikube/cert.pem, removing ...
I0201 22:08:21.149411   10079 exec_runner.go:203] rm: /home/haritha/.minikube/cert.pem
I0201 22:08:21.149559   10079 exec_runner.go:151] cp: /home/haritha/.minikube/certs/cert.pem --> /home/haritha/.minikube/cert.pem (1123 bytes)
I0201 22:08:21.149935   10079 exec_runner.go:144] found /home/haritha/.minikube/key.pem, removing ...
I0201 22:08:21.149941   10079 exec_runner.go:203] rm: /home/haritha/.minikube/key.pem
I0201 22:08:21.149971   10079 exec_runner.go:151] cp: /home/haritha/.minikube/certs/key.pem --> /home/haritha/.minikube/key.pem (1675 bytes)
I0201 22:08:21.150278   10079 provision.go:117] generating server cert: /home/haritha/.minikube/machines/server.pem ca-key=/home/haritha/.minikube/certs/ca.pem private-key=/home/haritha/.minikube/certs/ca-key.pem org=haritha.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0201 22:08:21.238694   10079 provision.go:177] copyRemoteCerts
I0201 22:08:21.238747   10079 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0201 22:08:21.238786   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:21.255496   10079 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/haritha/.minikube/machines/minikube/id_rsa Username:docker}
I0201 22:08:21.347631   10079 ssh_runner.go:362] scp /home/haritha/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0201 22:08:21.374626   10079 ssh_runner.go:362] scp /home/haritha/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0201 22:08:21.395131   10079 ssh_runner.go:362] scp /home/haritha/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0201 22:08:21.413924   10079 provision.go:87] duration metric: took 288.308442ms to configureAuth
I0201 22:08:21.413940   10079 ubuntu.go:193] setting minikube options for container-runtime
I0201 22:08:21.414093   10079 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0201 22:08:21.414131   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:21.430513   10079 main.go:141] libmachine: Using SSH client type: native
I0201 22:08:21.430658   10079 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0201 22:08:21.430663   10079 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0201 22:08:21.559765   10079 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0201 22:08:21.559785   10079 ubuntu.go:71] root file system type: overlay
I0201 22:08:21.560057   10079 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0201 22:08:21.560156   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:21.578669   10079 main.go:141] libmachine: Using SSH client type: native
I0201 22:08:21.578800   10079 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0201 22:08:21.578838   10079 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0201 22:08:21.730790   10079 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0201 22:08:21.730932   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:21.748846   10079 main.go:141] libmachine: Using SSH client type: native
I0201 22:08:21.749013   10079 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0201 22:08:21.749027   10079 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0201 22:08:21.907056   10079 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0201 22:08:21.907074   10079 machine.go:96] duration metric: took 1.247636582s to provisionDockerMachine
I0201 22:08:21.907267   10079 start.go:293] postStartSetup for "minikube" (driver="docker")
I0201 22:08:21.907280   10079 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0201 22:08:21.907349   10079 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0201 22:08:21.907391   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:21.922342   10079 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/haritha/.minikube/machines/minikube/id_rsa Username:docker}
I0201 22:08:22.023399   10079 ssh_runner.go:195] Run: cat /etc/os-release
I0201 22:08:22.028067   10079 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0201 22:08:22.028110   10079 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0201 22:08:22.028123   10079 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0201 22:08:22.028134   10079 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0201 22:08:22.028195   10079 filesync.go:126] Scanning /home/haritha/.minikube/addons for local assets ...
I0201 22:08:22.028638   10079 filesync.go:126] Scanning /home/haritha/.minikube/files for local assets ...
I0201 22:08:22.028773   10079 start.go:296] duration metric: took 121.496122ms for postStartSetup
I0201 22:08:22.028936   10079 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0201 22:08:22.028981   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:22.049345   10079 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/haritha/.minikube/machines/minikube/id_rsa Username:docker}
I0201 22:08:22.154865   10079 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0201 22:08:22.161248   10079 fix.go:56] duration metric: took 1.528791089s for fixHost
I0201 22:08:22.161267   10079 start.go:83] releasing machines lock for "minikube", held for 1.528946981s
I0201 22:08:22.161383   10079 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0201 22:08:22.176579   10079 ssh_runner.go:195] Run: cat /version.json
I0201 22:08:22.176618   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:22.177198   10079 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0201 22:08:22.177313   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:22.197000   10079 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/haritha/.minikube/machines/minikube/id_rsa Username:docker}
I0201 22:08:22.197115   10079 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/haritha/.minikube/machines/minikube/id_rsa Username:docker}
I0201 22:08:22.608953   10079 ssh_runner.go:195] Run: systemctl --version
I0201 22:08:22.612903   10079 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0201 22:08:22.617394   10079 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0201 22:08:22.636380   10079 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0201 22:08:22.636453   10079 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0201 22:08:22.644752   10079 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0201 22:08:22.644774   10079 start.go:495] detecting cgroup driver to use...
I0201 22:08:22.644830   10079 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0201 22:08:22.645751   10079 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0201 22:08:22.659840   10079 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0201 22:08:22.669597   10079 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0201 22:08:22.678982   10079 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0201 22:08:22.679060   10079 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0201 22:08:22.689078   10079 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0201 22:08:22.698666   10079 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0201 22:08:22.708872   10079 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0201 22:08:22.718127   10079 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0201 22:08:22.730136   10079 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0201 22:08:22.739373   10079 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0201 22:08:22.747133   10079 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0201 22:08:22.757953   10079 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0201 22:08:22.769419   10079 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0201 22:08:22.777852   10079 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0201 22:08:22.951706   10079 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0201 22:08:33.482537   10079 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.530774231s)
I0201 22:08:33.482650   10079 start.go:495] detecting cgroup driver to use...
I0201 22:08:33.482700   10079 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0201 22:08:33.482879   10079 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0201 22:08:33.496031   10079 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0201 22:08:33.496179   10079 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0201 22:08:33.508975   10079 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0201 22:08:33.525069   10079 ssh_runner.go:195] Run: which cri-dockerd
I0201 22:08:33.530677   10079 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0201 22:08:33.540380   10079 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0201 22:08:33.559301   10079 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0201 22:08:33.698026   10079 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0201 22:08:33.800962   10079 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0201 22:08:33.801158   10079 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0201 22:08:33.821363   10079 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0201 22:08:33.918719   10079 ssh_runner.go:195] Run: sudo systemctl restart docker
I0201 22:08:37.747026   10079 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.828270423s)
I0201 22:08:37.747085   10079 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0201 22:08:37.761638   10079 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0201 22:08:37.784627   10079 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0201 22:08:37.798112   10079 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0201 22:08:37.856516   10079 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0201 22:08:37.966589   10079 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0201 22:08:38.093262   10079 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0201 22:08:38.110873   10079 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0201 22:08:38.124095   10079 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0201 22:08:38.251219   10079 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0201 22:08:38.453958   10079 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0201 22:08:38.454125   10079 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0201 22:08:38.458149   10079 start.go:563] Will wait 60s for crictl version
I0201 22:08:38.458204   10079 ssh_runner.go:195] Run: which crictl
I0201 22:08:38.461634   10079 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0201 22:08:38.496332   10079 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I0201 22:08:38.496376   10079 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0201 22:08:38.520873   10079 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0201 22:08:38.543229   10079 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I0201 22:08:38.543506   10079 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0201 22:08:38.564475   10079 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0201 22:08:38.567870   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0201 22:08:38.587209   10079 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/haritha:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0201 22:08:38.587457   10079 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0201 22:08:38.587533   10079 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0201 22:08:38.604582   10079 docker.go:685] Got preloaded images: -- stdout --
harithamurugan/sam32:v1
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0201 22:08:38.604607   10079 docker.go:615] Images already preloaded, skipping extraction
I0201 22:08:38.604734   10079 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0201 22:08:38.620275   10079 docker.go:685] Got preloaded images: -- stdout --
harithamurugan/sam32:v1
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0201 22:08:38.620397   10079 cache_images.go:84] Images are preloaded, skipping loading
I0201 22:08:38.620439   10079 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I0201 22:08:38.620842   10079 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0201 22:08:38.620991   10079 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0201 22:08:38.773367   10079 cni.go:84] Creating CNI manager for ""
I0201 22:08:38.773384   10079 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0201 22:08:38.773448   10079 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0201 22:08:38.773495   10079 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0201 22:08:38.773654   10079 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0201 22:08:38.773717   10079 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I0201 22:08:38.785025   10079 binaries.go:44] Found k8s binaries, skipping transfer
I0201 22:08:38.785077   10079 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0201 22:08:38.793871   10079 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0201 22:08:38.818256   10079 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0201 22:08:38.876056   10079 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0201 22:08:38.900039   10079 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0201 22:08:38.905075   10079 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0201 22:08:39.476087   10079 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0201 22:08:39.581706   10079 certs.go:68] Setting up /home/haritha/.minikube/profiles/minikube for IP: 192.168.49.2
I0201 22:08:39.581831   10079 certs.go:194] generating shared ca certs ...
I0201 22:08:39.581862   10079 certs.go:226] acquiring lock for ca certs: {Name:mk32e58d5498326e91cd20b8ce5fce731a7a5da3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0201 22:08:39.582436   10079 certs.go:235] skipping valid "minikubeCA" ca cert: /home/haritha/.minikube/ca.key
I0201 22:08:39.582613   10079 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/haritha/.minikube/proxy-client-ca.key
I0201 22:08:39.582625   10079 certs.go:256] generating profile certs ...
I0201 22:08:39.582851   10079 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/haritha/.minikube/profiles/minikube/client.key
I0201 22:08:39.583063   10079 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/haritha/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0201 22:08:39.583281   10079 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/haritha/.minikube/profiles/minikube/proxy-client.key
I0201 22:08:39.583626   10079 certs.go:484] found cert: /home/haritha/.minikube/certs/ca-key.pem (1679 bytes)
I0201 22:08:39.583761   10079 certs.go:484] found cert: /home/haritha/.minikube/certs/ca.pem (1078 bytes)
I0201 22:08:39.583800   10079 certs.go:484] found cert: /home/haritha/.minikube/certs/cert.pem (1123 bytes)
I0201 22:08:39.583825   10079 certs.go:484] found cert: /home/haritha/.minikube/certs/key.pem (1675 bytes)
I0201 22:08:39.587593   10079 ssh_runner.go:362] scp /home/haritha/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0201 22:08:39.773770   10079 ssh_runner.go:362] scp /home/haritha/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0201 22:08:39.879739   10079 ssh_runner.go:362] scp /home/haritha/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0201 22:08:40.181548   10079 ssh_runner.go:362] scp /home/haritha/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0201 22:08:40.385713   10079 ssh_runner.go:362] scp /home/haritha/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0201 22:08:40.591928   10079 ssh_runner.go:362] scp /home/haritha/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0201 22:08:40.978032   10079 ssh_runner.go:362] scp /home/haritha/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0201 22:08:41.389303   10079 ssh_runner.go:362] scp /home/haritha/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0201 22:08:41.870185   10079 ssh_runner.go:362] scp /home/haritha/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0201 22:08:42.179877   10079 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0201 22:08:42.371750   10079 ssh_runner.go:195] Run: openssl version
I0201 22:08:42.484453   10079 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0201 22:08:42.769516   10079 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0201 22:08:42.783814   10079 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Dec  8 15:33 /usr/share/ca-certificates/minikubeCA.pem
I0201 22:08:42.784070   10079 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0201 22:08:42.875645   10079 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0201 22:08:42.975715   10079 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0201 22:08:42.988692   10079 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0201 22:08:43.167806   10079 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0201 22:08:43.274791   10079 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0201 22:08:43.467869   10079 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0201 22:08:43.574074   10079 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0201 22:08:43.682787   10079 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0201 22:08:43.769258   10079 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/haritha:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0201 22:08:43.769568   10079 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0201 22:08:44.068468   10079 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0201 22:08:44.171833   10079 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0201 22:08:44.172013   10079 kubeadm.go:593] restartPrimaryControlPlane start ...
I0201 22:08:44.172137   10079 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0201 22:08:44.273576   10079 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0201 22:08:44.273661   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0201 22:08:44.381078   10079 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:32776"
I0201 22:08:44.383670   10079 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0201 22:08:44.470083   10079 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0201 22:08:44.470228   10079 kubeadm.go:597] duration metric: took 298.207056ms to restartPrimaryControlPlane
I0201 22:08:44.470238   10079 kubeadm.go:394] duration metric: took 701.054779ms to StartCluster
I0201 22:08:44.470255   10079 settings.go:142] acquiring lock: {Name:mkb0ef2fe8c4100863f24abc481af931a757906e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0201 22:08:44.470318   10079 settings.go:150] Updating kubeconfig:  /home/haritha/.kube/config
I0201 22:08:44.470894   10079 lock.go:35] WriteFile acquiring /home/haritha/.kube/config: {Name:mkab2bf557421b7756f7be74a52d4af9cb929e6e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0201 22:08:44.471295   10079 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0201 22:08:44.471672   10079 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0201 22:08:44.471868   10079 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0201 22:08:44.471923   10079 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0201 22:08:44.471976   10079 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0201 22:08:44.472058   10079 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0201 22:08:44.472064   10079 addons.go:243] addon storage-provisioner should already be in state true
I0201 22:08:44.472124   10079 host.go:66] Checking if "minikube" exists ...
I0201 22:08:44.472141   10079 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0201 22:08:44.472426   10079 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0201 22:08:44.472452   10079 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0201 22:08:44.477317   10079 out.go:177] 🔎  Verifying Kubernetes components...
I0201 22:08:44.482710   10079 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0201 22:08:44.508786   10079 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0201 22:08:44.508796   10079 addons.go:243] addon default-storageclass should already be in state true
I0201 22:08:44.508820   10079 host.go:66] Checking if "minikube" exists ...
I0201 22:08:44.509100   10079 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0201 22:08:44.517860   10079 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0201 22:08:44.520481   10079 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0201 22:08:44.520496   10079 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0201 22:08:44.520795   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:44.535497   10079 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I0201 22:08:44.535516   10079 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0201 22:08:44.535614   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0201 22:08:44.550277   10079 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/haritha/.minikube/machines/minikube/id_rsa Username:docker}
I0201 22:08:44.551966   10079 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/haritha/.minikube/machines/minikube/id_rsa Username:docker}
I0201 22:08:45.269373   10079 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0201 22:08:45.281965   10079 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0201 22:08:45.287037   10079 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0201 22:08:51.504755   10079 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (6.222703392s)
I0201 22:08:51.504765   10079 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (6.235240927s)
I0201 22:08:51.505278   10079 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (6.218198455s)
I0201 22:08:51.505439   10079 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0201 22:08:51.526651   10079 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0201 22:08:51.528719   10079 addons.go:510] duration metric: took 7.056963432s for enable addons: enabled=[storage-provisioner default-storageclass]
I0201 22:08:51.534264   10079 api_server.go:52] waiting for apiserver process to appear ...
I0201 22:08:51.534365   10079 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0201 22:08:51.551335   10079 api_server.go:72] duration metric: took 7.079940957s to wait for apiserver process to appear ...
I0201 22:08:51.551366   10079 api_server.go:88] waiting for apiserver healthz status ...
I0201 22:08:51.551419   10079 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32776/healthz ...
I0201 22:08:51.557271   10079 api_server.go:279] https://127.0.0.1:32776/healthz returned 200:
ok
I0201 22:08:51.558553   10079 api_server.go:141] control plane version: v1.31.0
I0201 22:08:51.558568   10079 api_server.go:131] duration metric: took 7.197501ms to wait for apiserver health ...
I0201 22:08:51.558787   10079 system_pods.go:43] waiting for kube-system pods to appear ...
I0201 22:08:51.565578   10079 system_pods.go:59] 7 kube-system pods found
I0201 22:08:51.565627   10079 system_pods.go:61] "coredns-6f6b679f8f-dl9j8" [58fe710f-141d-4843-8a65-44c9584d16ff] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0201 22:08:51.565632   10079 system_pods.go:61] "etcd-minikube" [09e48f44-1cd1-4f59-bc4c-8b364b2e5de6] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0201 22:08:51.565640   10079 system_pods.go:61] "kube-apiserver-minikube" [e5858285-49cb-4be2-8774-567aa627a739] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0201 22:08:51.565643   10079 system_pods.go:61] "kube-controller-manager-minikube" [a5a28e79-ca12-4874-a4be-5a1b68b751b0] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0201 22:08:51.565645   10079 system_pods.go:61] "kube-proxy-cdnz6" [b493df0c-bc92-429f-b226-deecbd235bee] Running
I0201 22:08:51.565647   10079 system_pods.go:61] "kube-scheduler-minikube" [49a5c28f-90b1-4c90-82c8-14226f1ee857] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0201 22:08:51.565648   10079 system_pods.go:61] "storage-provisioner" [6fb76c06-7bd9-422e-b4d3-bd48adfa79f2] Running
I0201 22:08:51.565654   10079 system_pods.go:74] duration metric: took 6.862351ms to wait for pod list to return data ...
I0201 22:08:51.565661   10079 kubeadm.go:582] duration metric: took 7.094326997s to wait for: map[apiserver:true system_pods:true]
I0201 22:08:51.565700   10079 node_conditions.go:102] verifying NodePressure condition ...
I0201 22:08:51.569914   10079 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0201 22:08:51.570007   10079 node_conditions.go:123] node cpu capacity is 8
I0201 22:08:51.570109   10079 node_conditions.go:105] duration metric: took 4.403666ms to run NodePressure ...
I0201 22:08:51.570124   10079 start.go:241] waiting for startup goroutines ...
I0201 22:08:51.570128   10079 start.go:246] waiting for cluster config update ...
I0201 22:08:51.570136   10079 start.go:255] writing updated cluster config ...
I0201 22:08:51.570503   10079 ssh_runner.go:195] Run: rm -f paused
I0201 22:08:51.697261   10079 start.go:600] kubectl: 1.31.3, cluster: 1.31.0 (minor skew: 0)
I0201 22:08:51.700167   10079 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 01 22:08:35 minikube dockerd[7763]: time="2025-02-01T22:08:35.505434688Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Feb 01 22:08:35 minikube dockerd[7763]: time="2025-02-01T22:08:35.505474241Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Feb 01 22:08:35 minikube dockerd[7763]: time="2025-02-01T22:08:35.505479398Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Feb 01 22:08:35 minikube dockerd[7763]: time="2025-02-01T22:08:35.505482633Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 01 22:08:35 minikube dockerd[7763]: time="2025-02-01T22:08:35.505499822Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Feb 01 22:08:35 minikube dockerd[7763]: time="2025-02-01T22:08:35.505559605Z" level=info msg="Daemon has completed initialization"
Feb 01 22:08:35 minikube dockerd[7763]: time="2025-02-01T22:08:35.649739331Z" level=info msg="API listen on /var/run/docker.sock"
Feb 01 22:08:35 minikube dockerd[7763]: time="2025-02-01T22:08:35.649785827Z" level=info msg="API listen on [::]:2376"
Feb 01 22:08:35 minikube dockerd[7763]: time="2025-02-01T22:08:35.652119306Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Feb 01 22:08:35 minikube dockerd[7763]: time="2025-02-01T22:08:35.653187470Z" level=info msg="Daemon shutdown complete"
Feb 01 22:08:35 minikube systemd[1]: docker.service: Deactivated successfully.
Feb 01 22:08:35 minikube systemd[1]: Stopped Docker Application Container Engine.
Feb 01 22:08:35 minikube systemd[1]: Starting Docker Application Container Engine...
Feb 01 22:08:35 minikube dockerd[8045]: time="2025-02-01T22:08:35.704958365Z" level=info msg="Starting up"
Feb 01 22:08:35 minikube dockerd[8045]: time="2025-02-01T22:08:35.729016164Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Feb 01 22:08:35 minikube dockerd[8045]: time="2025-02-01T22:08:35.739461652Z" level=info msg="Loading containers: start."
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.370931288Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.590638027Z" level=warning msg="error locating sandbox id 785209b1b8a847684e787ccdddb6f688c88c5a124f5a90a622850d440cd9d467: sandbox 785209b1b8a847684e787ccdddb6f688c88c5a124f5a90a622850d440cd9d467 not found"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.590786515Z" level=warning msg="error locating sandbox id 0d76a7031ee51fb17fd18772974f80e80f8041742c40ac649abd26d512a02a3c: sandbox 0d76a7031ee51fb17fd18772974f80e80f8041742c40ac649abd26d512a02a3c not found"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.590839241Z" level=warning msg="error locating sandbox id 52e2e54f5be65178b672fd5e2928fdf904dbc26d6fb8aca0b6a6309b0679c3a7: sandbox 52e2e54f5be65178b672fd5e2928fdf904dbc26d6fb8aca0b6a6309b0679c3a7 not found"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.590892105Z" level=warning msg="error locating sandbox id 4e837699ad2c1531c389551b1e27d4bbfdbda1d586aea46093ee03ce775dae93: sandbox 4e837699ad2c1531c389551b1e27d4bbfdbda1d586aea46093ee03ce775dae93 not found"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.590950240Z" level=warning msg="error locating sandbox id 0c01975f5d9a5c8ae16fdd7eb97569689675fb070506d74ac48030ec064ac542: sandbox 0c01975f5d9a5c8ae16fdd7eb97569689675fb070506d74ac48030ec064ac542 not found"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.590996276Z" level=warning msg="error locating sandbox id 86438be5fad27394d1e52f7afc344cd1f3052d6d6874cda645f5c5de9a4ad9a9: sandbox 86438be5fad27394d1e52f7afc344cd1f3052d6d6874cda645f5c5de9a4ad9a9 not found"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.591920135Z" level=info msg="Loading containers: done."
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.609778260Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.609823060Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.609831829Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.609836891Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.609860049Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.609921590Z" level=info msg="Daemon has completed initialization"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.743663911Z" level=info msg="API listen on /var/run/docker.sock"
Feb 01 22:08:37 minikube dockerd[8045]: time="2025-02-01T22:08:37.743780653Z" level=info msg="API listen on [::]:2376"
Feb 01 22:08:37 minikube systemd[1]: Started Docker Application Container Engine.
Feb 01 22:08:37 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
Feb 01 22:08:37 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Feb 01 22:08:37 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Feb 01 22:08:38 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Start docker client with request timeout 0s"
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Hairpin mode is set to hairpin-veth"
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Loaded network plugin cni"
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Docker cri networking managed by network plugin cni"
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Setting cgroupDriver cgroupfs"
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Start cri-dockerd grpc backend"
Feb 01 22:08:38 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"demo1-5d9fdfb596-fvwkf_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a020070fdf4657c8006f304b90929f9363eafdb4ba40a61eb2eb54e7d47f6aa7\""
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-dl9j8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b32147bd4737de616fce76f5cd13d8eba0453a0d0a2944758582222ee4d91388\""
Feb 01 22:08:38 minikube cri-dockerd[8355]: time="2025-02-01T22:08:38Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-dl9j8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"086084ad3a4bbf0b37c3ada3c4abf2b6d27485bf24e30603c28b7d97499bb7c3\""
Feb 01 22:08:39 minikube cri-dockerd[8355]: time="2025-02-01T22:08:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f735d3c5e2ea4d06aff23357b313c2d6562c71ecf7132b08bcd667ddf1bb3a69/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 01 22:08:40 minikube cri-dockerd[8355]: time="2025-02-01T22:08:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bb2d480c91b2e24bac135a807b49acca4caab0f3b4a49fd0d87bc559f5735a6e/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 01 22:08:40 minikube cri-dockerd[8355]: time="2025-02-01T22:08:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e68ff78a0caab8bef59aa2246d20bf90839226123164fd44201af2560b64463f/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 01 22:08:40 minikube cri-dockerd[8355]: time="2025-02-01T22:08:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ea4958052c6a5a0a21cda793e5395b3e1ba0be1d4041012d78b60c74ee138dd8/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 01 22:08:40 minikube cri-dockerd[8355]: time="2025-02-01T22:08:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/07fb28064218a5c926060d7a7eae61c7a33a5db80258fca34663fcb1cda64152/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 01 22:08:40 minikube cri-dockerd[8355]: time="2025-02-01T22:08:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/37507506fc65b3e03bcb8735bd68e134fae1d0d702c89d53c69777cdffb79545/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 01 22:08:41 minikube cri-dockerd[8355]: time="2025-02-01T22:08:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/479add243df4dca0fb985793eb4d82f69f062003a66f55c9da402367da94054f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 01 22:08:41 minikube cri-dockerd[8355]: time="2025-02-01T22:08:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6fee4997ddacfb34a37f50bada9908967330d4a15d80a7c75c6a97ab5cc8b7fe/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 01 22:09:04 minikube dockerd[8045]: time="2025-02-01T22:09:04.706881434Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
34a64e4ddf908       cbb01a7bd410d       2 minutes ago       Running             coredns                   12                  6fee4997ddacf       coredns-6f6b679f8f-dl9j8
b93a8941998eb       6e38f40d628db       2 minutes ago       Running             storage-provisioner       24                  37507506fc65b       storage-provisioner
67bbca8ad9b47       ad83b2ca7b09e       2 minutes ago       Running             kube-proxy                12                  07fb28064218a       kube-proxy-cdnz6
978d0bb6d7969       045733566833c       2 minutes ago       Running             kube-controller-manager   12                  ea4958052c6a5       kube-controller-manager-minikube
9ebad263cc925       1766f54c897f0       2 minutes ago       Running             kube-scheduler            12                  e68ff78a0caab       kube-scheduler-minikube
2bc3fe51da02e       2e96e5913fc06       2 minutes ago       Running             etcd                      12                  bb2d480c91b2e       etcd-minikube
674fff6f9c8f7       604f5db92eaa8       2 minutes ago       Running             kube-apiserver            12                  f735d3c5e2ea4       kube-apiserver-minikube
793acde08ea8f       6e38f40d628db       17 minutes ago      Exited              storage-provisioner       23                  0a4ff7eec93a0       storage-provisioner
c724cac3d26ba       cbb01a7bd410d       18 minutes ago      Exited              coredns                   11                  b32147bd4737d       coredns-6f6b679f8f-dl9j8
59868f777067c       ad83b2ca7b09e       18 minutes ago      Exited              kube-proxy                11                  2b2508341888d       kube-proxy-cdnz6
7287d2e142a9b       045733566833c       18 minutes ago      Exited              kube-controller-manager   11                  04b51c6151a85       kube-controller-manager-minikube
e63239d8e7a49       1766f54c897f0       18 minutes ago      Exited              kube-scheduler            11                  a2928963f3d90       kube-scheduler-minikube
a9dcb806230f3       2e96e5913fc06       18 minutes ago      Exited              etcd                      11                  fb025c78f43f7       etcd-minikube
897b994298649       604f5db92eaa8       18 minutes ago      Exited              kube-apiserver            11                  0fc6c60912080       kube-apiserver-minikube


==> coredns [34a64e4ddf90] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:44902 - 49944 "HINFO IN 3969099385467246306.6373540205251293745. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.400741971s


==> coredns [c724cac3d26b] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:40072 - 29495 "HINFO IN 6086415884027524811.858149671437912316. udp 56 false 512" NXDOMAIN qr,rd,ra 131 0.094737904s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1411402342]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (01-Feb-2025 21:53:08.356) (total time: 30003ms):
Trace[1411402342]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (21:53:38.360)
Trace[1411402342]: [30.003787129s] [30.003787129s] END
[INFO] plugin/kubernetes: Trace[1639330915]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (01-Feb-2025 21:53:08.356) (total time: 30003ms):
Trace[1639330915]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (21:53:38.360)
Trace[1639330915]: [30.003818428s] [30.003818428s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[802552407]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (01-Feb-2025 21:53:08.356) (total time: 30003ms):
Trace[802552407]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (21:53:38.360)
Trace[802552407]: [30.003935851s] [30.003935851s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_01_17T18_56_22_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 17 Jan 2025 18:56:19 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 01 Feb 2025 22:11:32 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 01 Feb 2025 22:08:44 +0000   Fri, 17 Jan 2025 18:56:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 01 Feb 2025 22:08:44 +0000   Fri, 17 Jan 2025 18:56:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 01 Feb 2025 22:08:44 +0000   Fri, 17 Jan 2025 18:56:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 01 Feb 2025 22:08:44 +0000   Fri, 17 Jan 2025 18:56:19 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3945240Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3945240Ki
  pods:               110
System Info:
  Machine ID:                 a4f77959afcc4fd6afeecf633d888d98
  System UUID:                a4f77959afcc4fd6afeecf633d888d98
  Boot ID:                    a1c96393-bfdb-415a-9541-76ce4c930827
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     demo1-5d9fdfb596-fvwkf              0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m2s
  kube-system                 coredns-6f6b679f8f-dl9j8            100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     15d
  kube-system                 etcd-minikube                       100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         15d
  kube-system                 kube-apiserver-minikube             250m (3%)     0 (0%)      0 (0%)           0 (0%)         15d
  kube-system                 kube-controller-manager-minikube    200m (2%)     0 (0%)      0 (0%)           0 (0%)         15d
  kube-system                 kube-proxy-cdnz6                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15d
  kube-system                 kube-scheduler-minikube             100m (1%)     0 (0%)      0 (0%)           0 (0%)         15d
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         15d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           2m46s              kube-proxy       
  Normal   Starting                           18m                kube-proxy       
  Normal   Starting                           58m                kube-proxy       
  Normal   RegisteredNode                     58m                node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  CgroupV1                           18m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   Starting                           18m                kubelet          Starting kubelet.
  Warning  PossibleMemoryBackedVolumesOnDisk  18m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   NodeHasSufficientMemory            18m (x7 over 18m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              18m (x7 over 18m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               18m (x7 over 18m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            18m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     18m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode                     2m42s              node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.907318] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.016468] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001562] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001293] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001025] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Feb 1 19:36] Exception: 
[  +0.000014] Operation canceled @p9io.cpp:258 (AcceptAsync)

[Feb 1 19:44] Adjusting tsc more than 11% (7777241 vs 7697885)
[ +33.020495] FS-Cache: Duplicate cookie detected
[  +0.000963] FS-Cache: O-cookie c=00000035 [p=00000002 fl=222 nc=0 na=1]
[  +0.001341] FS-Cache: O-cookie d=00000000dc42c250{9P.session} n=000000009d76f136
[  +0.002062] FS-Cache: O-key=[10] '34323935353832383639'
[  +0.001173] FS-Cache: N-cookie c=00000036 [p=00000002 fl=2 nc=0 na=1]
[  +0.000987] FS-Cache: N-cookie d=00000000dc42c250{9P.session} n=000000009a569376
[  +0.001055] FS-Cache: N-key=[10] '34323935353832383639'
[  +0.292599] FS-Cache: Duplicate cookie detected
[  +0.000813] FS-Cache: O-cookie c=00000039 [p=00000002 fl=222 nc=0 na=1]
[  +0.000872] FS-Cache: O-cookie d=00000000dc42c250{9P.session} n=000000003bc68995
[  +0.002334] FS-Cache: O-key=[10] '34323935353832383939'
[  +0.001586] FS-Cache: N-cookie c=0000003a [p=00000002 fl=2 nc=0 na=1]
[  +0.000809] FS-Cache: N-cookie d=00000000dc42c250{9P.session} n=00000000594ecbe4
[  +0.000786] FS-Cache: N-key=[10] '34323935353832383939'
[  +0.234625] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.003343] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000921] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000656] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000799] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Feb 1 19:45] Exception: 
[  +0.000011] Operation canceled @p9io.cpp:258 (AcceptAsync)

[Feb 1 21:23] Exception: 
[  +0.000017] Operation canceled @p9io.cpp:258 (AcceptAsync)

[  +3.781752] FS-Cache: Duplicate cookie detected
[  +0.005442] FS-Cache: O-cookie c=00000040 [p=00000002 fl=222 nc=0 na=1]
[  +0.000681] FS-Cache: O-cookie d=00000000dc42c250{9P.session} n=00000000619a987c
[  +0.001608] FS-Cache: O-key=[10] '34323936313736373737'
[  +0.001805] FS-Cache: N-cookie c=00000041 [p=00000002 fl=2 nc=0 na=1]
[  +0.000746] FS-Cache: N-cookie d=00000000dc42c250{9P.session} n=0000000017ef219f
[  +0.000648] FS-Cache: N-key=[10] '34323936313736373737'
[  +0.541547] FS-Cache: Duplicate cookie detected
[  +0.001644] FS-Cache: O-cookie c=00000045 [p=00000002 fl=222 nc=0 na=1]
[  +0.000681] FS-Cache: O-cookie d=00000000dc42c250{9P.session} n=00000000bb247f97
[  +0.001040] FS-Cache: O-key=[10] '34323936313736383332'
[  +0.000745] FS-Cache: N-cookie c=00000046 [p=00000002 fl=2 nc=0 na=1]
[  +0.000546] FS-Cache: N-cookie d=00000000dc42c250{9P.session} n=00000000b11a4874
[  +0.000715] FS-Cache: N-key=[10] '34323936313736383332'
[  +0.022209] WSL (2) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.390978] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.008324] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000663] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000441] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000517] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.036167] Failed to connect to bus: No such file or directory
[  +0.281356] Failed to connect to bus: No such file or directory
[  +0.914467] systemd-journald[65]: File /var/log/journal/daa95ffe7ac2415c92cb395473c6e6cd/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +8.501490] WSL (2) ERROR: WaitForBootProcess:3352: /sbin/init failed to start within 10000
[  +0.000037] ms
[Feb 1 21:53] tmpfs: Unknown parameter 'noswap'


==> etcd [2bc3fe51da02] <==
{"level":"warn","ts":"2025-02-01T22:08:42.369165Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-02-01T22:08:42.369577Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-02-01T22:08:42.370276Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-02-01T22:08:42.370522Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-02-01T22:08:42.370563Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-01T22:08:42.370929Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-01T22:08:42.376026Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-02-01T22:08:42.376907Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-02-01T22:08:42.387289Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"8.995858ms"}
{"level":"info","ts":"2025-02-01T22:08:46.498466Z","caller":"etcdserver/server.go:511","msg":"recovered v2 store from snapshot","snapshot-index":60006,"snapshot-size":"10 kB"}
{"level":"info","ts":"2025-02-01T22:08:46.498576Z","caller":"etcdserver/server.go:524","msg":"recovered v3 backend from snapshot","backend-size-bytes":3747840,"backend-size":"3.7 MB","backend-size-in-use-bytes":1495040,"backend-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-02-01T22:08:46.975012Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":69166}
{"level":"info","ts":"2025-02-01T22:08:46.975567Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-02-01T22:08:46.975897Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 13"}
{"level":"info","ts":"2025-02-01T22:08:46.975922Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 13, commit: 69166, applied: 60006, lastindex: 69166, lastterm: 13]"}
{"level":"info","ts":"2025-02-01T22:08:46.976214Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-01T22:08:46.976406Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-01T22:08:46.976461Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2025-02-01T22:08:46.978487Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-02-01T22:08:46.980203Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":55029}
{"level":"info","ts":"2025-02-01T22:08:46.983540Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":55376}
{"level":"info","ts":"2025-02-01T22:08:46.986836Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-02-01T22:08:46.992186Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-02-01T22:08:46.993406Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-02-01T22:08:46.993535Z","caller":"etcdserver/server.go:858","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-01T22:08:46.994057Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-02-01T22:08:46.994551Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-01T22:08:46.995021Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-01T22:08:46.995110Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-01T22:08:46.995955Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-01T22:08:47.000479Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-01T22:08:47.000768Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-01T22:08:47.000855Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-01T22:08:47.001161Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-02-01T22:08:47.001237Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-02-01T22:08:47.877409Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 13"}
{"level":"info","ts":"2025-02-01T22:08:47.877568Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 13"}
{"level":"info","ts":"2025-02-01T22:08:47.878891Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 13"}
{"level":"info","ts":"2025-02-01T22:08:47.878982Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 14"}
{"level":"info","ts":"2025-02-01T22:08:47.879009Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 14"}
{"level":"info","ts":"2025-02-01T22:08:47.879035Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 14"}
{"level":"info","ts":"2025-02-01T22:08:47.879055Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 14"}
{"level":"info","ts":"2025-02-01T22:08:47.889138Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-02-01T22:08:47.889255Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-01T22:08:47.889379Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-01T22:08:47.891040Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-02-01T22:08:47.891096Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-02-01T22:08:47.894762Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-01T22:08:47.894903Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-01T22:08:47.895847Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-02-01T22:08:47.896178Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-02-01T22:08:49.271402Z","caller":"traceutil/trace.go:171","msg":"trace[1074071473] transaction","detail":"{read_only:false; response_revision:55378; number_of_response:1; }","duration":"103.541659ms","start":"2025-02-01T22:08:49.167770Z","end":"2025-02-01T22:08:49.271312Z","steps":["trace[1074071473] 'process raft request'  (duration: 17.587829ms)","trace[1074071473] 'compare'  (duration: 85.455753ms)"],"step_count":2}
{"level":"info","ts":"2025-02-01T22:08:49.273103Z","caller":"traceutil/trace.go:171","msg":"trace[2028891171] transaction","detail":"{read_only:false; response_revision:55379; number_of_response:1; }","duration":"105.089422ms","start":"2025-02-01T22:08:49.167990Z","end":"2025-02-01T22:08:49.273079Z","steps":["trace[2028891171] 'process raft request'  (duration: 102.968129ms)"],"step_count":1}


==> etcd [a9dcb806230f] <==
{"level":"info","ts":"2025-02-01T21:53:03.309320Z","caller":"etcdserver/server.go:511","msg":"recovered v2 store from snapshot","snapshot-index":60006,"snapshot-size":"10 kB"}
{"level":"info","ts":"2025-02-01T21:53:03.309393Z","caller":"etcdserver/server.go:524","msg":"recovered v3 backend from snapshot","backend-size-bytes":3391488,"backend-size":"3.4 MB","backend-size-in-use-bytes":1032192,"backend-size-in-use":"1.0 MB"}
{"level":"info","ts":"2025-02-01T21:53:04.087722Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":68042}
{"level":"info","ts":"2025-02-01T21:53:04.088689Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-02-01T21:53:04.089104Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 12"}
{"level":"info","ts":"2025-02-01T21:53:04.089346Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 12, commit: 68042, applied: 60006, lastindex: 68042, lastterm: 12]"}
{"level":"info","ts":"2025-02-01T21:53:04.089723Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-01T21:53:04.089759Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-01T21:53:04.089775Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2025-02-01T21:53:04.091420Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-02-01T21:53:04.092859Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":54209}
{"level":"info","ts":"2025-02-01T21:53:04.096370Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":54453}
{"level":"info","ts":"2025-02-01T21:53:04.099195Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-02-01T21:53:04.101663Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-02-01T21:53:04.102040Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-02-01T21:53:04.102120Z","caller":"etcdserver/server.go:858","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-01T21:53:04.102423Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-02-01T21:53:04.102762Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-01T21:53:04.104268Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-01T21:53:04.104380Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-01T21:53:04.104392Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-01T21:53:04.109222Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-01T21:53:04.109579Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-01T21:53:04.110704Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-01T21:53:04.168206Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-02-01T21:53:04.168489Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-02-01T21:53:04.490191Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 12"}
{"level":"info","ts":"2025-02-01T21:53:04.490282Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 12"}
{"level":"info","ts":"2025-02-01T21:53:04.490341Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 12"}
{"level":"info","ts":"2025-02-01T21:53:04.490363Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 13"}
{"level":"info","ts":"2025-02-01T21:53:04.490371Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 13"}
{"level":"info","ts":"2025-02-01T21:53:04.490383Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 13"}
{"level":"info","ts":"2025-02-01T21:53:04.490393Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 13"}
{"level":"info","ts":"2025-02-01T21:53:04.494984Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-02-01T21:53:04.495052Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-01T21:53:04.495062Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-01T21:53:04.495458Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-02-01T21:53:04.495569Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-02-01T21:53:04.497636Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-01T21:53:04.497785Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-01T21:53:04.498623Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-02-01T21:53:04.498787Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-02-01T22:01:30.048659Z","caller":"wal/wal.go:785","msg":"created a new WAL segment","path":"/var/lib/minikube/etcd/member/wal/0000000000000001-0000000000010c0c.wal"}
{"level":"info","ts":"2025-02-01T22:03:04.530740Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54757}
{"level":"info","ts":"2025-02-01T22:03:04.544456Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":54757,"took":"11.744993ms","hash":1814183336,"current-db-size-bytes":3510272,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":2408448,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-02-01T22:03:04.544574Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1814183336,"revision":54757,"compact-revision":54209}
{"level":"info","ts":"2025-02-01T22:08:04.546481Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":55029}
{"level":"info","ts":"2025-02-01T22:08:04.555762Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":55029,"took":"8.926101ms","hash":2001201573,"current-db-size-bytes":3747840,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":2260992,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-02-01T22:08:04.555812Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2001201573,"revision":55029,"compact-revision":54757}
{"level":"info","ts":"2025-02-01T22:08:23.185580Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-02-01T22:08:23.191499Z","caller":"embed/etcd.go:377","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-02-01T22:08:23.274632Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-02-01T22:08:23.275264Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
2025/02/01 22:08:23 WARNING: [core] [Server #7] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2025-02-01T22:08:23.367402Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-02-01T22:08:23.367516Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-02-01T22:08:23.368373Z","caller":"etcdserver/server.go:1521","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-02-01T22:08:23.468777Z","caller":"embed/etcd.go:581","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-01T22:08:23.470028Z","caller":"embed/etcd.go:586","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-01T22:08:23.470129Z","caller":"embed/etcd.go:379","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 22:11:35 up  4:14,  0 users,  load average: 0.73, 0.68, 0.38
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [674fff6f9c8f] <==
W0201 22:08:48.410746       1 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0201 22:08:48.411523       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0201 22:08:48.411556       1 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0201 22:08:48.424337       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0201 22:08:48.424384       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0201 22:08:48.903603       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0201 22:08:48.903626       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0201 22:08:48.903797       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0201 22:08:48.905533       1 secure_serving.go:213] Serving securely on [::]:8443
I0201 22:08:48.905737       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0201 22:08:48.905819       1 controller.go:119] Starting legacy_token_tracking_controller
I0201 22:08:48.905850       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0201 22:08:48.905865       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0201 22:08:48.906002       1 local_available_controller.go:156] Starting LocalAvailability controller
I0201 22:08:48.906023       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0201 22:08:48.906048       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0201 22:08:48.906053       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0201 22:08:48.906086       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0201 22:08:48.906121       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0201 22:08:48.906091       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0201 22:08:48.906025       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0201 22:08:48.906323       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0201 22:08:48.906417       1 controller.go:142] Starting OpenAPI controller
I0201 22:08:48.906450       1 controller.go:90] Starting OpenAPI V3 controller
I0201 22:08:48.906466       1 naming_controller.go:294] Starting NamingConditionController
I0201 22:08:48.906481       1 establishing_controller.go:81] Starting EstablishingController
I0201 22:08:48.906516       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0201 22:08:48.906520       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0201 22:08:48.906540       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0201 22:08:48.906572       1 crd_finalizer.go:269] Starting CRDFinalizer
I0201 22:08:48.906744       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I0201 22:08:48.906755       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0201 22:08:48.906805       1 aggregator.go:169] waiting for initial CRD sync...
I0201 22:08:48.906818       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0201 22:08:48.906861       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0201 22:08:48.906937       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0201 22:08:48.907108       1 controller.go:78] Starting OpenAPI AggregationController
I0201 22:08:48.907216       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0201 22:08:48.907225       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0201 22:08:49.066719       1 cache.go:39] Caches are synced for LocalAvailability controller
I0201 22:08:49.067332       1 shared_informer.go:320] Caches are synced for node_authorizer
I0201 22:08:49.067928       1 shared_informer.go:320] Caches are synced for configmaps
I0201 22:08:49.069585       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0201 22:08:49.069696       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0201 22:08:49.071145       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0201 22:08:49.071174       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0201 22:08:49.071181       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0201 22:08:49.071438       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0201 22:08:49.071474       1 aggregator.go:171] initial CRD sync complete...
I0201 22:08:49.071484       1 autoregister_controller.go:144] Starting autoregister controller
I0201 22:08:49.071493       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0201 22:08:49.071505       1 cache.go:39] Caches are synced for autoregister controller
I0201 22:08:49.075133       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0201 22:08:49.075233       1 policy_source.go:224] refreshing policies
I0201 22:08:49.080196       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I0201 22:08:49.090300       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0201 22:08:49.984756       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0201 22:08:51.487278       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0201 22:08:51.489469       1 controller.go:615] quota admission added evaluator for: endpoints
I0201 22:08:51.500405       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io


==> kube-apiserver [897b99429864] <==
W0201 22:08:28.718582       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:28.765601       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:28.773280       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:28.809383       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:28.826352       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:28.837284       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:28.844346       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:28.999453       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:28.999600       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:28.999825       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:29.035117       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:29.044403       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:29.055659       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:29.064144       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:29.115665       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:31.391188       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:31.445495       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:31.563542       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:31.638563       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:31.756355       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:31.821914       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:31.850598       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:31.923505       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:31.982851       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:31.993844       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.072314       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.129509       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.149373       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.264749       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.270944       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.293737       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.406560       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.421227       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.423745       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.437072       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.445614       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.503983       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.527437       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.563276       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.572061       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.598900       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.639029       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.650677       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.663523       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.701703       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.705012       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.757268       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.774333       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.788524       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.841237       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.848828       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.863079       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.867999       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.919167       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.940118       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:32.960878       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:33.049453       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:33.055101       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:33.083750       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0201 22:08:33.086183       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [7287d2e142a9] <==
I0201 21:53:09.496267       1 shared_informer.go:320] Caches are synced for HPA
I0201 21:53:09.504618       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0201 21:53:09.508238       1 shared_informer.go:320] Caches are synced for GC
I0201 21:53:09.515200       1 shared_informer.go:320] Caches are synced for TTL
I0201 21:53:09.518928       1 shared_informer.go:320] Caches are synced for crt configmap
I0201 21:53:09.533192       1 shared_informer.go:320] Caches are synced for persistent volume
I0201 21:53:09.533595       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0201 21:53:09.533751       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0201 21:53:09.534208       1 shared_informer.go:320] Caches are synced for ReplicationController
I0201 21:53:09.534427       1 shared_informer.go:320] Caches are synced for stateful set
I0201 21:53:09.534444       1 shared_informer.go:320] Caches are synced for endpoint
I0201 21:53:09.535454       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0201 21:53:09.540338       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0201 21:53:09.542247       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0201 21:53:09.583950       1 shared_informer.go:320] Caches are synced for attach detach
I0201 21:53:09.601982       1 shared_informer.go:320] Caches are synced for namespace
I0201 21:53:09.610869       1 shared_informer.go:320] Caches are synced for service account
I0201 21:53:09.637499       1 shared_informer.go:320] Caches are synced for cronjob
I0201 21:53:09.694846       1 shared_informer.go:320] Caches are synced for resource quota
I0201 21:53:09.756352       1 shared_informer.go:320] Caches are synced for resource quota
I0201 21:53:09.783116       1 shared_informer.go:320] Caches are synced for taint
I0201 21:53:09.783368       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0201 21:53:09.783519       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0201 21:53:09.783569       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0201 21:53:09.788488       1 shared_informer.go:320] Caches are synced for daemon sets
I0201 21:53:09.845696       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="303.324192ms"
I0201 21:53:09.846779       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="117.749µs"
I0201 21:53:10.169959       1 shared_informer.go:320] Caches are synced for garbage collector
I0201 21:53:10.227499       1 shared_informer.go:320] Caches are synced for garbage collector
I0201 21:53:10.227541       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0201 21:53:43.987024       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="10.017235ms"
I0201 21:53:43.987433       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="83.58µs"
I0201 21:58:12.205680       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0201 22:02:19.788486       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="44.485457ms"
I0201 22:02:19.797736       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="9.090496ms"
I0201 22:02:19.799310       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="63.854µs"
I0201 22:02:19.812741       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="67.481µs"
I0201 22:02:19.893442       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="76.095µs"
I0201 22:02:30.185339       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="42.78µs"
I0201 22:02:41.698195       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="369.372µs"
I0201 22:02:58.696743       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="58.49µs"
I0201 22:03:10.699824       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="381.676µs"
I0201 22:03:18.018985       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0201 22:03:31.695932       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="48.032µs"
I0201 22:03:45.693553       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="168.454µs"
I0201 22:04:15.690618       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="111.504µs"
I0201 22:04:27.695730       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="581.606µs"
I0201 22:05:33.601495       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="8.1µs"
I0201 22:05:33.979203       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="32.729694ms"
I0201 22:05:33.991844       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="12.54387ms"
I0201 22:05:33.991988       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="65.691µs"
I0201 22:05:34.003448       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="58.786µs"
I0201 22:05:42.100293       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="61.12µs"
I0201 22:05:55.699043       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="45.181µs"
I0201 22:06:15.692061       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="448.448µs"
I0201 22:06:27.691939       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="99µs"
I0201 22:06:48.695531       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="138.454µs"
I0201 22:06:59.696319       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="52.753µs"
I0201 22:07:42.693828       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="103.161µs"
I0201 22:07:54.694460       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="175.216µs"


==> kube-controller-manager [978d0bb6d796] <==
I0201 22:08:53.250099       1 shared_informer.go:313] Waiting for caches to sync for ReplicationController
I0201 22:08:53.258387       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0201 22:08:53.269049       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0201 22:08:53.269851       1 shared_informer.go:320] Caches are synced for PVC protection
I0201 22:08:53.269987       1 shared_informer.go:320] Caches are synced for disruption
I0201 22:08:53.271191       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0201 22:08:53.271362       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="115.247µs"
I0201 22:08:53.279030       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0201 22:08:53.296608       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0201 22:08:53.296643       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0201 22:08:53.296666       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0201 22:08:53.296627       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0201 22:08:53.296795       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0201 22:08:53.298677       1 shared_informer.go:320] Caches are synced for daemon sets
I0201 22:08:53.300083       1 shared_informer.go:320] Caches are synced for endpoint
I0201 22:08:53.300133       1 shared_informer.go:320] Caches are synced for service account
I0201 22:08:53.300505       1 shared_informer.go:320] Caches are synced for node
I0201 22:08:53.300570       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I0201 22:08:53.300587       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I0201 22:08:53.300592       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0201 22:08:53.300597       1 shared_informer.go:320] Caches are synced for cidrallocator
I0201 22:08:53.300648       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0201 22:08:53.300757       1 shared_informer.go:320] Caches are synced for PV protection
I0201 22:08:53.302365       1 shared_informer.go:320] Caches are synced for job
I0201 22:08:53.304148       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0201 22:08:53.304695       1 shared_informer.go:320] Caches are synced for cronjob
I0201 22:08:53.304863       1 shared_informer.go:320] Caches are synced for namespace
I0201 22:08:53.306708       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0201 22:08:53.310037       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0201 22:08:53.312974       1 shared_informer.go:320] Caches are synced for deployment
I0201 22:08:53.313022       1 shared_informer.go:320] Caches are synced for attach detach
I0201 22:08:53.314621       1 shared_informer.go:320] Caches are synced for GC
I0201 22:08:53.316137       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0201 22:08:53.323092       1 shared_informer.go:320] Caches are synced for expand
I0201 22:08:53.325687       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0201 22:08:53.329504       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0201 22:08:53.347817       1 shared_informer.go:320] Caches are synced for stateful set
I0201 22:08:53.347905       1 shared_informer.go:320] Caches are synced for TTL after finished
I0201 22:08:53.349405       1 shared_informer.go:320] Caches are synced for ephemeral
I0201 22:08:53.349487       1 shared_informer.go:320] Caches are synced for TTL
I0201 22:08:53.350878       1 shared_informer.go:320] Caches are synced for ReplicationController
I0201 22:08:53.351011       1 shared_informer.go:320] Caches are synced for HPA
I0201 22:08:53.355570       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0201 22:08:53.358463       1 shared_informer.go:320] Caches are synced for crt configmap
I0201 22:08:53.361603       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="90.207909ms"
I0201 22:08:53.361845       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="64.222µs"
I0201 22:08:53.410013       1 shared_informer.go:320] Caches are synced for persistent volume
I0201 22:08:53.517993       1 shared_informer.go:320] Caches are synced for taint
I0201 22:08:53.518114       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0201 22:08:53.518318       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0201 22:08:53.518601       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0201 22:08:53.558655       1 shared_informer.go:320] Caches are synced for resource quota
I0201 22:08:53.592235       1 shared_informer.go:320] Caches are synced for resource quota
I0201 22:08:53.981540       1 shared_informer.go:320] Caches are synced for garbage collector
I0201 22:08:53.996244       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="16.173933ms"
I0201 22:08:53.996421       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="51.464µs"
I0201 22:08:54.066994       1 shared_informer.go:320] Caches are synced for garbage collector
I0201 22:08:54.067060       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0201 22:09:15.693431       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="46.754µs"
I0201 22:09:29.695162       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/demo1-5d9fdfb596" duration="57.038µs"


==> kube-proxy [59868f777067] <==
E0201 21:53:08.218577       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0201 21:53:08.225394       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0201 21:53:08.242780       1 server_linux.go:66] "Using iptables proxy"
I0201 21:53:08.590168       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0201 21:53:08.591029       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0201 21:53:08.646929       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0201 21:53:08.647061       1 server_linux.go:169] "Using iptables Proxier"
I0201 21:53:08.651322       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0201 21:53:08.662135       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0201 21:53:08.670112       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0201 21:53:08.670262       1 server.go:483] "Version info" version="v1.31.0"
I0201 21:53:08.670279       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0201 21:53:08.673346       1 config.go:104] "Starting endpoint slice config controller"
I0201 21:53:08.673523       1 config.go:197] "Starting service config controller"
I0201 21:53:08.673848       1 config.go:326] "Starting node config controller"
I0201 21:53:08.674625       1 shared_informer.go:313] Waiting for caches to sync for service config
I0201 21:53:08.674635       1 shared_informer.go:313] Waiting for caches to sync for node config
I0201 21:53:08.674624       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0201 21:53:08.775479       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0201 21:53:08.775521       1 shared_informer.go:320] Caches are synced for service config
I0201 21:53:08.775549       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [67bbca8ad9b4] <==
E0201 22:08:44.384832       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0201 22:08:44.395835       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0201 22:08:44.569796       1 server_linux.go:66] "Using iptables proxy"
I0201 22:08:49.082826       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0201 22:08:49.083131       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0201 22:08:49.401637       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0201 22:08:49.401973       1 server_linux.go:169] "Using iptables Proxier"
I0201 22:08:49.471708       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0201 22:08:49.492477       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0201 22:08:49.567209       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0201 22:08:49.577103       1 server.go:483] "Version info" version="v1.31.0"
I0201 22:08:49.577200       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0201 22:08:49.588727       1 config.go:197] "Starting service config controller"
I0201 22:08:49.588965       1 config.go:104] "Starting endpoint slice config controller"
I0201 22:08:49.593085       1 config.go:326] "Starting node config controller"
I0201 22:08:49.667457       1 shared_informer.go:313] Waiting for caches to sync for service config
I0201 22:08:49.667470       1 shared_informer.go:313] Waiting for caches to sync for node config
I0201 22:08:49.667470       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0201 22:08:49.667500       1 shared_informer.go:320] Caches are synced for service config
I0201 22:08:49.667506       1 shared_informer.go:320] Caches are synced for node config
I0201 22:08:49.667520       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [9ebad263cc92] <==
I0201 22:08:45.882244       1 serving.go:386] Generated self-signed cert in-memory
W0201 22:08:48.986442       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0201 22:08:48.986605       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0201 22:08:48.986645       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0201 22:08:48.986716       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0201 22:08:49.091174       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0201 22:08:49.091222       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0201 22:08:49.095564       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0201 22:08:49.095858       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0201 22:08:49.095964       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0201 22:08:49.096077       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0201 22:08:49.268243       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [e63239d8e7a4] <==
I0201 21:53:03.485604       1 serving.go:386] Generated self-signed cert in-memory
W0201 21:53:06.176732       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0201 21:53:06.177118       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found]
W0201 21:53:06.177347       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0201 21:53:06.177421       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0201 21:53:06.291356       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0201 21:53:06.291391       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0201 21:53:06.294398       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0201 21:53:06.294446       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0201 21:53:06.294472       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0201 21:53:06.294525       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0201 21:53:06.398715       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0201 22:08:23.078771       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0201 22:08:23.080613       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0201 22:08:23.082122       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E0201 22:08:23.082556       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Feb 01 22:08:39 minikube kubelet[1613]: I0201 22:08:39.075959    1613 status_manager.go:851] "Failed to get status for pod" podUID="58fe710f-141d-4843-8a65-44c9584d16ff" pod="kube-system/coredns-6f6b679f8f-dl9j8" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-dl9j8\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:39 minikube kubelet[1613]: I0201 22:08:39.076983    1613 status_manager.go:851] "Failed to get status for pod" podUID="b493df0c-bc92-429f-b226-deecbd235bee" pod="kube-system/kube-proxy-cdnz6" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-cdnz6\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:39 minikube kubelet[1613]: I0201 22:08:39.078645    1613 status_manager.go:851] "Failed to get status for pod" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d" pod="default/demo1-5d9fdfb596-fvwkf" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/demo1-5d9fdfb596-fvwkf\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:39 minikube kubelet[1613]: I0201 22:08:39.079231    1613 status_manager.go:851] "Failed to get status for pod" podUID="e039200acb850c82bb901653cc38ff6e" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:39 minikube kubelet[1613]: I0201 22:08:39.079625    1613 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:39 minikube kubelet[1613]: I0201 22:08:39.080887    1613 status_manager.go:851] "Failed to get status for pod" podUID="40f5f661ab65f2e4bfe41ac2993c01de" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:39 minikube kubelet[1613]: I0201 22:08:39.278680    1613 scope.go:117] "RemoveContainer" containerID="38dd6de5a641e0880c2bf9635ee046d18557c35d9ae77ee64e3bdc0ad787a25f"
Feb 01 22:08:39 minikube kubelet[1613]: I0201 22:08:39.566920    1613 scope.go:117] "RemoveContainer" containerID="2cb6c2880d60cbdc3c2e1c0a645b30bcd088d7bce2830e8df69449d96752839c"
Feb 01 22:08:39 minikube kubelet[1613]: I0201 22:08:39.683946    1613 scope.go:117] "RemoveContainer" containerID="5e2457022528d7de73f38ae0b20aa7ee46ca9fd0c988fdbcd6b2a587c009dba0"
Feb 01 22:08:40 minikube kubelet[1613]: I0201 22:08:40.679533    1613 status_manager.go:851] "Failed to get status for pod" podUID="40f5f661ab65f2e4bfe41ac2993c01de" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:40 minikube kubelet[1613]: I0201 22:08:40.680055    1613 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:40 minikube kubelet[1613]: I0201 22:08:40.680439    1613 status_manager.go:851] "Failed to get status for pod" podUID="58fe710f-141d-4843-8a65-44c9584d16ff" pod="kube-system/coredns-6f6b679f8f-dl9j8" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-dl9j8\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:40 minikube kubelet[1613]: I0201 22:08:40.680833    1613 status_manager.go:851] "Failed to get status for pod" podUID="b493df0c-bc92-429f-b226-deecbd235bee" pod="kube-system/kube-proxy-cdnz6" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-cdnz6\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:40 minikube kubelet[1613]: I0201 22:08:40.681195    1613 status_manager.go:851] "Failed to get status for pod" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d" pod="default/demo1-5d9fdfb596-fvwkf" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/demo1-5d9fdfb596-fvwkf\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:40 minikube kubelet[1613]: I0201 22:08:40.681551    1613 status_manager.go:851] "Failed to get status for pod" podUID="e039200acb850c82bb901653cc38ff6e" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:40 minikube kubelet[1613]: I0201 22:08:40.682928    1613 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:40 minikube kubelet[1613]: I0201 22:08:40.683725    1613 status_manager.go:851] "Failed to get status for pod" podUID="6fb76c06-7bd9-422e-b4d3-bd48adfa79f2" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:41 minikube kubelet[1613]: I0201 22:08:41.691570    1613 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="479add243df4dca0fb985793eb4d82f69f062003a66f55c9da402367da94054f"
Feb 01 22:08:42 minikube kubelet[1613]: I0201 22:08:42.073491    1613 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6fee4997ddacfb34a37f50bada9908967330d4a15d80a7c75c6a97ab5cc8b7fe"
Feb 01 22:08:42 minikube kubelet[1613]: E0201 22:08:42.773157    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:08:42 minikube kubelet[1613]: I0201 22:08:42.867340    1613 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="07fb28064218a5c926060d7a7eae61c7a33a5db80258fca34663fcb1cda64152"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.076958    1613 status_manager.go:851] "Failed to get status for pod" podUID="6fb76c06-7bd9-422e-b4d3-bd48adfa79f2" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.079030    1613 status_manager.go:851] "Failed to get status for pod" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d" pod="default/demo1-5d9fdfb596-fvwkf" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/demo1-5d9fdfb596-fvwkf\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.079965    1613 status_manager.go:851] "Failed to get status for pod" podUID="e039200acb850c82bb901653cc38ff6e" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.080982    1613 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.082942    1613 status_manager.go:851] "Failed to get status for pod" podUID="40f5f661ab65f2e4bfe41ac2993c01de" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.085381    1613 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.168896    1613 status_manager.go:851] "Failed to get status for pod" podUID="58fe710f-141d-4843-8a65-44c9584d16ff" pod="kube-system/coredns-6f6b679f8f-dl9j8" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-dl9j8\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.169552    1613 status_manager.go:851] "Failed to get status for pod" podUID="b493df0c-bc92-429f-b226-deecbd235bee" pod="kube-system/kube-proxy-cdnz6" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-cdnz6\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.368479    1613 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ea4958052c6a5a0a21cda793e5395b3e1ba0be1d4041012d78b60c74ee138dd8"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.369613    1613 status_manager.go:851] "Failed to get status for pod" podUID="6fb76c06-7bd9-422e-b4d3-bd48adfa79f2" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.369850    1613 status_manager.go:851] "Failed to get status for pod" podUID="e039200acb850c82bb901653cc38ff6e" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.370040    1613 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.370864    1613 status_manager.go:851] "Failed to get status for pod" podUID="40f5f661ab65f2e4bfe41ac2993c01de" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.371434    1613 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.371801    1613 status_manager.go:851] "Failed to get status for pod" podUID="58fe710f-141d-4843-8a65-44c9584d16ff" pod="kube-system/coredns-6f6b679f8f-dl9j8" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-dl9j8\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.372144    1613 status_manager.go:851] "Failed to get status for pod" podUID="b493df0c-bc92-429f-b226-deecbd235bee" pod="kube-system/kube-proxy-cdnz6" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-cdnz6\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.372486    1613 status_manager.go:851] "Failed to get status for pod" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d" pod="default/demo1-5d9fdfb596-fvwkf" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/demo1-5d9fdfb596-fvwkf\": dial tcp 192.168.49.2:8443: connect: connection refused"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.682734    1613 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="37507506fc65b3e03bcb8735bd68e134fae1d0d702c89d53c69777cdffb79545"
Feb 01 22:08:43 minikube kubelet[1613]: I0201 22:08:43.994852    1613 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="bb2d480c91b2e24bac135a807b49acca4caab0f3b4a49fd0d87bc559f5735a6e"
Feb 01 22:08:44 minikube kubelet[1613]: I0201 22:08:44.181657    1613 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e68ff78a0caab8bef59aa2246d20bf90839226123164fd44201af2560b64463f"
Feb 01 22:08:46 minikube kubelet[1613]: E0201 22:08:46.082660    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:08:47 minikube kubelet[1613]: E0201 22:08:47.105078    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:08:48 minikube kubelet[1613]: E0201 22:08:48.975740    1613 reflector.go:158] "Unhandled Error" err="object-\"kube-system\"/\"kube-root-ca.crt\": Failed to watch *v1.ConfigMap: unknown (get configmaps)" logger="UnhandledError"
Feb 01 22:09:02 minikube kubelet[1613]: I0201 22:09:02.169473    1613 scope.go:117] "RemoveContainer" containerID="711beb4bcd219c707932e26b361ebea56e2857325c31bb4a8bd0986881179b58"
Feb 01 22:09:04 minikube kubelet[1613]: E0201 22:09:04.713556    1613 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for harithamurugan/demo1:latest not found: manifest unknown: manifest unknown" image="harithamurugan/demo1:latest"
Feb 01 22:09:04 minikube kubelet[1613]: E0201 22:09:04.713658    1613 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: manifest for harithamurugan/demo1:latest not found: manifest unknown: manifest unknown" image="harithamurugan/demo1:latest"
Feb 01 22:09:04 minikube kubelet[1613]: E0201 22:09:04.713840    1613 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:demo1,Image:harithamurugan/demo1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5vl5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod demo1-5d9fdfb596-fvwkf_default(f985e9dc-9bc1-416a-b25a-31d6619cb73d): ErrImagePull: Error response from daemon: manifest for harithamurugan/demo1:latest not found: manifest unknown: manifest unknown" logger="UnhandledError"
Feb 01 22:09:04 minikube kubelet[1613]: E0201 22:09:04.715381    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ErrImagePull: \"Error response from daemon: manifest for harithamurugan/demo1:latest not found: manifest unknown: manifest unknown\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:09:15 minikube kubelet[1613]: E0201 22:09:15.685531    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:09:29 minikube kubelet[1613]: E0201 22:09:29.683642    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:09:43 minikube kubelet[1613]: E0201 22:09:43.683866    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:09:58 minikube kubelet[1613]: E0201 22:09:58.683784    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:10:11 minikube kubelet[1613]: E0201 22:10:11.681767    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:10:22 minikube kubelet[1613]: E0201 22:10:22.682109    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:10:35 minikube kubelet[1613]: E0201 22:10:35.684615    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:10:49 minikube kubelet[1613]: E0201 22:10:49.683350    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:11:02 minikube kubelet[1613]: E0201 22:11:02.684180    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:11:17 minikube kubelet[1613]: E0201 22:11:17.681880    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"
Feb 01 22:11:28 minikube kubelet[1613]: E0201 22:11:28.681455    1613 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo1\" with ImagePullBackOff: \"Back-off pulling image \\\"harithamurugan/demo1\\\"\"" pod="default/demo1-5d9fdfb596-fvwkf" podUID="f985e9dc-9bc1-416a-b25a-31d6619cb73d"


==> storage-provisioner [793acde08ea8] <==
I0201 21:53:49.788460       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0201 21:53:49.799340       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0201 21:53:49.799786       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0201 21:54:07.200630       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0201 21:54:07.200820       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_1550756c-488b-47c8-8434-8dd6b9c69b6c!
I0201 21:54:07.200810       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"81d84f70-588f-469b-b5be-a2d5c472d69c", APIVersion:"v1", ResourceVersion:"54567", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_1550756c-488b-47c8-8434-8dd6b9c69b6c became leader
I0201 21:54:07.302728       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_1550756c-488b-47c8-8434-8dd6b9c69b6c!


==> storage-provisioner [b93a8941998e] <==
I0201 22:08:43.390415       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0201 22:08:49.085921       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0201 22:08:49.086398       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0201 22:09:06.596857       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0201 22:09:06.598804       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_86461d78-28b4-4d79-b567-1c78ac2a4fa7!
I0201 22:09:06.599455       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"81d84f70-588f-469b-b5be-a2d5c472d69c", APIVersion:"v1", ResourceVersion:"55495", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_86461d78-28b4-4d79-b567-1c78ac2a4fa7 became leader
I0201 22:09:06.701689       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_86461d78-28b4-4d79-b567-1c78ac2a4fa7!

